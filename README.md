# Final Project
## Collaborators
Anil Bayram Gogebakan - s328470

Oguzhan Akgun - s328919

Meric Ulucay - s328899
## Introduction
In this project, our primary objective is to develop and apply symbolic regression methods to solve a given dataset while achieving the lowest Mean Squared Error (MSE) score. By minimizing the MSE, we ensure that the discovered equations not only capture the relationships within the data but also provide reliable predictive performance. To enhance the search efficiency and maintain a balance between exploration and exploitation, we incorporate techniques such as migration, aging, and the killing of eldest individuals, inspired by the approaches detailed in Cranmer et al.'s work on symbolic regression algorithms (https://arxiv.org/pdf/2305.01582). These methods allow us to prevent premature convergence, maintain diversity in the population, and systematically refine the solutions by leveraging evolutionary principles. I was involved in all the simplification processes of the project, focusing on reducing complexity and discovering better formulas to improve the results.


To begin with, we defined node class to represent our tree structure. Each node can be constant, operator that is listed in op_list (+,-,*,sin,cos,exp,abs,/,log,tan) or one of the features of x. If constant, value will be equal to that number. If operator, value will come from np.{operator}. If feature of x, we know that it is a feature, value will be None, and the index of feature will be stored in feature_index. There are left and right values to represent child nodes. If there is only one child node, only left value exists. We also make complexity calculation for each node. Nodes create individuals. Individuals have also train and validation cost as fitness and fitness_val which is equal to cost of that formula, age value which is initially set to 0 and T value for simulated annealing to be used in mutation process.

## Generations
In the evolve function, after randomly creating a population with the size of 100 individuals and calculating their fitness values, we used tournament selection to select the ones who will produce new individuals by organizing tournaments for 3 random individuals and eliminating individuals whose costs are the largest. In our code we defined elites’ number as 3 not to eliminate them during killing eldest process. We also implemented aging methodology to avoid local minimum, increase diversity and exploration. In generation creating process loop we used mutation and crossover methods with probability of %80 for each individual and increased age by 1 for each cycle.

## Simplification
We tried to simplify constants of genome in individuals for example instead of (2+3) directly will be shown as 5. Initially, we observed that the dataset had a low likelihood of being constant. However, we lost variation as the population started producing too many constants. To address this issue, we implemented a killing constants strategy, where individuals that are purely constants are directly removed from the population. This helped maintain the balance between exploration and exploitation within the algorithm and reducing the complexity. As part of the evolution process, we implemented a simplification operation to reduce unnecessary complexity in the mathematical expressions generated by individuals. Specifically, if numerical constants are used as arguments in mathematical functions like sin, cos, tan, or abs, we directly calculate their results. For example, instead of representing sin(π/2) or abs(-5), the algorithm simplifies them to their respective constant values 1 and 5. This optimization ensures that the individuals in the population maintain concise and computationally efficient expressions, preventing redundant calculations during fitness evaluation.

The results we generated were often quite lengthy, so we added functions to simplify the root expressions. For instance, an equation like log(x[0]) + log(x[0]) could be simplified to 2*log(x[0]). However, this simplification didn’t improve performance. In some cases, keeping the expressions unsimplified actually worked better for mutation and crossover, as it introduced more diversity for the algorithm to explore.

## Deduplication 
To enhance diversity within the population and avoid redundant evaluations, we implemented a deduplication mechanism. This process identifies individuals with identical genomes and removes duplicates, ensuring that only one individual with a unique genome remains in the population. By eliminating identical solutions, we reduce computational overhead and maintain a more diverse set of candidates, allowing the algorithm to focus on exploring new areas of the solution space.

To ensure computational efficiency and convergence, we implemented several mechanisms in the evolutionary process. When the dataset size doubles, we continue with the best 100 individuals to balance computational cost and maintain a diverse yet manageable population. Additionally, the evolution process halts if the cost of any individual becomes extremely small, such as below a threshold of 0.0001. This stopping criterion ensures that the algorithm terminates when an optimal or near-optimal solution is found, avoiding unnecessary computation in subsequent generations. By focusing on the top-performing individuals and introducing this convergence condition, we streamline the search process while maintaining high accuracy.

## Migration
To enhance diversity and improve the optimization process, we utilized a multi-population strategy with 4 separate populations. Each population evolves independently using the methods described earlier, including selection, mutation, crossover, aging, and deduplication. At specific intervals, migration is performed within each population, where individuals are exchanged to share genetic information and prevent stagnation. By incorporating this multi-population approach with migration, we ensure a balance between exploration and exploitation, allowing the algorithm to avoid local minima and find the most accurate and interpretable solution for the dataset. After migration, each population continues to evolve further. This cycle of evolve → migration → evolve repeats until the stopping criteria are met. Once the evolution process concludes, the best individual from each of the 4 populations is selected. Finally, the overall best individual among these is chosen as the optimal solution for the problem.

